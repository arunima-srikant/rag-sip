# RAG-based Document Query System

This repository contains a project at **breakWalls Technologies** as a part of a summer internship, where I developed a **Retrieval-Augmented Generation (RAG)** pipeline to enable document-aware question answering. The system allows users to query product documentation and receive accurate, context-grounded responses generated by a local language model.

---

## Project Overview

Organizations like breakWalls manage extensive technical documentation, and answering queries from this knowledge base is often manual and time-consuming. The goal of this project was to:

* Automate document-based query resolution.
* Ensure data privacy by running models locally (offline, no external APIs).
* Experiment with different chunking, embedding, and retrieval strategies for performance optimization.

---

## Pipeline Design

The RAG pipeline is built with the following components:

1. **Document Chunking**

   * Splits large documents into smaller, meaningful text chunks.
   * Enables efficient search and retrieval.

2. **Embedding Generation**

   * Converts chunks into vector embeddings representing semantic meaning.
   * Used cosine similarity to measure closeness between query and document chunks.

3. **Retrieval & Response Generation**

   * Retrieves top relevant chunks based on query similarity.
   * Passes retrieved context to a local LLM (via **Ollama**) to generate responses.

---

## Tools & Frameworks

* **Python**
* [LlamaIndex](https://github.com/run-llama/llama_index) – for document ingestion and retrieval.
* [Ollama](https://ollama.ai/) – for running local LLMs.
* **cosine similarity** – for embedding-based search.

---

## Learning Outcomes

Through this project, I:

* Gained hands-on experience with building RAG pipelines.
* Understood how embeddings, chunk sizes, and retrieval strategies impact performance.
* Explored trade-offs between fine-tuning, RAG, and RAFT (Retrieval-Augmented Fine-Tuning).
* Improved debugging, documentation, and independent problem-solving skills.

---

## Future Work

* Improve chunking strategies for better retrieval granularity.
* Optimize local model performance for faster response times.
* Explore integration with a front-end interface.

---

## Keywords

`RAG` `LLM` `LlamaIndex` `Ollama` `Embeddings` `Document Retrieval` `AI Applications`
